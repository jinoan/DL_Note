{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T09:52:30.489400Z",
     "start_time": "2021-04-22T09:52:30.486141Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\")))\n",
    "from nlp.preprocessing import jamo_spliter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T07:18:57.617069Z",
     "start_time": "2021-04-22T07:15:23.612592Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "r = open(\"../../datasets/extractive_summary/train.jsonl\", \"r\", encoding=\"utf8\")\n",
    "w = open(\"../../datasets/extractive_summary/train_hg.json\", \"a\", encoding=\"utf8\")\n",
    "c = open(\"../../datasets/extractive_summary/corpus.txt\", \"a\", encoding=\"utf8\")\n",
    "try:\n",
    "    count = 0\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line: break\n",
    "        count += 1\n",
    "        line = json.loads(line)\n",
    "        for i, s in enumerate(line[\"article_original\"]):\n",
    "            line[\"article_original\"][i] = hg_encoder.encode(s)\n",
    "            c.write(line[\"article_original\"][i] + \"\\n\")\n",
    "        line[\"abstractive\"] = hg_encoder.encode(line[\"abstractive\"])\n",
    "        c.write(line[\"abstractive\"] + \"\\n\")\n",
    "        w.write(json.dumps(line, ensure_ascii=False))\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"█{count}\")\n",
    "        elif count % 100 == 0:\n",
    "            print(\"█\", end=\"\")\n",
    "    print(f\"data size: {count}\")\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    r.close()\n",
    "    w.close()\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T07:34:33.169122Z",
     "start_time": "2021-04-22T07:34:32.355232Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"../../datasets/extractive_summary/corpus.txt\", \"r\", encoding=\"utf8\") as c:\n",
    "    dataset = c.readlines()\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make WordPiece tokenizer\n",
    "\n",
    "https://velog.io/@nawnoes/Huggingface-tokenizers%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-Wordpiece-Tokenizer-%EB%A7%8C%EB%93%A4%EA%B8%B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T09:34:48.872629Z",
     "start_time": "2021-04-22T09:34:48.870146Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T09:34:54.681282Z",
     "start_time": "2021-04-22T09:34:54.678943Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T09:35:05.411540Z",
     "start_time": "2021-04-22T09:35:05.409678Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T09:35:31.575731Z",
     "start_time": "2021-04-22T09:35:31.573325Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T09:38:35.402003Z",
     "start_time": "2021-04-22T09:38:27.499824Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "files = [\"../../datasets/extractive_summary/corpus.txt\"]\n",
    "bert_tokenizer.train(files, trainer)\n",
    "\n",
    "bert_tokenizer.save(\"../../datasets/extractive_summary/wpm.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T09:42:48.138071Z",
     "start_time": "2021-04-22T09:42:48.131813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ㅇㅣ', '-', 'ㄱㅓ', '-', 'ㅈㅏㄹ', 'ㄴㅏ', '-', 'ㅇㅗ', '-', 'ㄴㅏ', '-', 'ㅇㅛ', '--?-', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ㅇㅣ - ㄱㅓ - ㅈㅏㄹ ㄴㅏ - ㅇㅗ - ㄴㅏ - ㅇㅛ --?-'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bert_tokenizer.encode(hg_encoder.encode(\"이거 잘 나오나요?\"))\n",
    "print(output.tokens)\n",
    "# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\n",
    "\n",
    "bert_tokenizer.decode(output.ids)\n",
    "# \"Hello , y ' all ! How are you ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
