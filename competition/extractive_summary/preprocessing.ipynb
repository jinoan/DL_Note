{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:02:04.100648Z",
     "start_time": "2021-04-22T15:02:03.686135Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"..\")))\n",
    "from nlp.preprocessing import bert_jamo_spliter\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:55:50.169912Z",
     "start_time": "2021-04-22T14:52:24.159634Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "████████████████████████████████████████████████████████████████████████████████████████████████████10000\n",
      "████████████████████████████████████████████████████████████████████████████████████████████████████20000\n",
      "████████████████████████████████████████████████████████████████████████████████████████████████████30000\n",
      "████████████████████████████████████████████████████████████████████████████████████████████████████40000\n",
      "████████████████████████████42803\n"
     ]
    }
   ],
   "source": [
    "r = open(\"../../datasets/extractive_summary/train.jsonl\", \"r\", encoding=\"utf8\")\n",
    "w = open(\"../../datasets/extractive_summary/train_hg.json\", \"a\", encoding=\"utf8\")\n",
    "c = open(\"../../datasets/extractive_summary/corpus.txt\", \"a\", encoding=\"utf8\")\n",
    "try:\n",
    "    count = 0\n",
    "    while True:\n",
    "        line = r.readline()\n",
    "        if not line: break\n",
    "        count += 1\n",
    "        line = json.loads(line)\n",
    "        for i, s in enumerate(line[\"article_original\"]):\n",
    "            line[\"article_original\"][i] = bert_jamo_spliter.encode(s)\n",
    "            c.write(line[\"article_original\"][i] + \"\\n\")\n",
    "        line[\"abstractive\"] = bert_jamo_spliter.encode(line[\"abstractive\"])\n",
    "        c.write(line[\"abstractive\"] + \"\\n\")\n",
    "        w.write(json.dumps(line, ensure_ascii=False))\n",
    "        if count % 10000 == 0:\n",
    "            print(f\"█{count}\")\n",
    "        elif count % 100 == 0:\n",
    "            print(\"█\", end=\"\")\n",
    "    print(f\"{count}\")\n",
    "except Exception as e:\n",
    "    raise e\n",
    "finally:\n",
    "    r.close()\n",
    "    w.close()\n",
    "    c.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:57:16.116647Z",
     "start_time": "2021-04-22T14:57:15.515208Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "610225"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../../datasets/extractive_summary/corpus.txt\", \"r\", encoding=\"utf8\") as c:\n",
    "    dataset = c.readlines()\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make WordPiece tokenizer\n",
    "\n",
    "https://velog.io/@nawnoes/Huggingface-tokenizers%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-Wordpiece-Tokenizer-%EB%A7%8C%EB%93%A4%EA%B8%B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:57:19.574274Z",
     "start_time": "2021-04-22T14:57:19.512008Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:57:20.376797Z",
     "start_time": "2021-04-22T14:57:20.373401Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:57:21.774829Z",
     "start_time": "2021-04-22T14:57:21.772418Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:57:22.439647Z",
     "start_time": "2021-04-22T14:57:22.436617Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[\n",
    "        (\"[CLS]\", 1),\n",
    "        (\"[SEP]\", 2),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:57:47.590021Z",
     "start_time": "2021-04-22T14:57:23.497524Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=30522, special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
    ")\n",
    "files = [\"../../datasets/extractive_summary/corpus.txt\"]\n",
    "bert_tokenizer.train(files, trainer)\n",
    "\n",
    "bert_tokenizer.save(\"../../datasets/extractive_summary/wpm.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T14:57:50.236042Z",
     "start_time": "2021-04-22T14:57:50.222026Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'ㄷㅏㅇㅈㅣㄴㅅㅣ', 'ㅁㅜㄴㅎㅘㄱㅘㄴㄱㅘㅇ', '##ㄱㅘㄹㅡㄹ', 'ㄷㅐㅅㅏㅇㅇㅡㄹㅗ', 'ㅎㅏㄴㅡㄴ', 'ㅎㅐㅇㅈㅓㅇㅅㅏㅁㅜㄱㅏㅁㅅㅏㅇㅔㅅㅓ', 'ㄷㅏㅇㅈㅣㄴㅅㅣㄹㅣㅂ', '##ㅎㅏㅂㅊㅏㅇㄷㅏㄴ', 'ㄱㅘㄴㄱㅖㅈㅏㄱㅏ', 'ㅂㅗㄴㅐㄴ', 'ㄱㅓㅅㅇㅡㄹㅗ', 'ㅊㅜㅈㅓㅇㄷㅚㄴㅡㄴ', 'ㅁㅜㄴㅈㅏ', '##ㅇㅘ', 'ㄱㅘㄴㄹㅕㄴㅎㅐ', 'ㄷㅏㅇㅈㅣㄴㅅㅣㅇㅢㅎㅚ', '##ㄱㅏ', 'ㅎㅐㅇㅈㅓㅇㅅㅏㅁㅜ', '##ㅈㅗㅅㅏ', '##ㅌㅡㄱㅂㅕㄹㅇㅟㅇㅝㄴㅎㅚ', '##ㄹㅡㄹ', 'ㄱㅜㅅㅓㅇㅎㅐ', 'ㅈㅗㅅㅏ', '##ㅎㅏㄱㅔㅆㄷㅏㄱㅗ', 'ㅂㅏㄺㅎㅕㅆㄷㅏ', '.', '[SEP]']\n",
      "당진 문화관 과 대상으 하 행정사무감사에 당진시 합창 관계자 보 것으 추정되 문  관련 당진시의  행정사 조 특별위원  구성 조 하겠다 밝혔 .\n"
     ]
    }
   ],
   "source": [
    "output = bert_tokenizer.encode(bert_jamo_spliter.encode(\"당진시 문화관광과를 대상으로 하는 행정사무감사에서 당진시립합창단 관계자가 보낸 것으로 추정되는 문자와 관련해 당진시의회가 행정사무조사특별위원회를 구성해 조사하겠다고 밝혔다.\"))\n",
    "print(output.tokens)\n",
    "# [1, 27253, 16, 93, 11, 5097, 5, 7961, 5112, 6218, 0, 35, 2]\n",
    "\n",
    "recover = bert_tokenizer.decode(output.ids)\n",
    "recover = recover.replace('#', '')\n",
    "print(bert_jamo_spliter.decode(recover))\n",
    "# \"Hello , y ' all ! How are you ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-22T15:02:10.358124Z",
     "start_time": "2021-04-22T15:02:10.352149Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'문화관광'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_jamo_spliter.decode(\"ㅁㅜㄴㅎㅘㄱㅘㄴㄱㅘㅇ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
